import { createRequire } from 'module';
import fs from 'fs';
import path from 'path';
import { fileURLToPath } from 'url';

const require = createRequire(import.meta.url);
const __dirname = path.dirname(fileURLToPath(import.meta.url));
const ROOT = path.dirname(__dirname);
const DATA_DIR = path.join(ROOT, 'data');

const SPEAKER_EMBEDDINGS_URL = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/speaker_embeddings.bin';
const SPEAKER_EMBEDDINGS_PATH = path.join(DATA_DIR, 'speaker_embeddings.bin');
const SAMPLE_RATE_TTS = 16000;
const SAMPLE_RATE_STT = 16000;
const MIN_WAV_SIZE = 44;
const DATASET_API = 'https://datasets-server.huggingface.co/rows?dataset=Matthijs%2Fcmu-arctic-xvectors&config=default&split=validation';
const SAMPLES_TO_AVERAGE = 10;

const VOICE_CATALOG = [
  { id: 'default', name: 'Default', gender: 'male', accent: 'US' },
  { id: 'bdl', name: 'BDL', gender: 'male', accent: 'US' },
  { id: 'slt', name: 'SLT', gender: 'female', accent: 'US' },
  { id: 'clb', name: 'CLB', gender: 'female', accent: 'US' },
  { id: 'rms', name: 'RMS', gender: 'male', accent: 'US' },
  { id: 'awb', name: 'AWB', gender: 'male', accent: 'Scottish' },
  { id: 'jmk', name: 'JMK', gender: 'male', accent: 'Canadian' },
  { id: 'ksp', name: 'KSP', gender: 'male', accent: 'Indian' },
];

const SPEAKER_OFFSETS = { awb: 0, bdl: 1200, clb: 2300, jmk: 3500, ksp: 4700, rms: 5900, slt: 7100 };

let transformersModule = null;
let sttPipeline = null;
let ttsPipeline = null;
let speakerEmbeddings = null;
let sttLoading = false;
let ttsLoading = false;
const voiceEmbeddingsCache = new Map();

const TTS_CACHE_MAX = 100;
const ttsCache = new Map();

async function loadTransformers() {
  if (transformersModule) return transformersModule;
  transformersModule = await import('@huggingface/transformers');
  return transformersModule;
}

function whisperModelPath() {
  try {
    const webtalkDir = path.dirname(require.resolve('webtalk'));
    const p = path.join(webtalkDir, 'models', 'onnx-community', 'whisper-base');
    if (fs.existsSync(p)) return p;
  } catch (_) {}
  return 'onnx-community/whisper-base';
}

async function ensureSpeakerEmbeddings() {
  if (speakerEmbeddings) return speakerEmbeddings;
  if (!fs.existsSync(DATA_DIR)) fs.mkdirSync(DATA_DIR, { recursive: true });
  if (!fs.existsSync(SPEAKER_EMBEDDINGS_PATH)) {
    const resp = await fetch(SPEAKER_EMBEDDINGS_URL);
    if (!resp.ok) throw new Error('Failed to download speaker embeddings');
    fs.writeFileSync(SPEAKER_EMBEDDINGS_PATH, Buffer.from(await resp.arrayBuffer()));
  }
  const buf = fs.readFileSync(SPEAKER_EMBEDDINGS_PATH);
  speakerEmbeddings = new Float32Array(new Uint8Array(buf).buffer);
  return speakerEmbeddings;
}

async function loadVoiceEmbedding(voiceId) {
  if (!voiceId || voiceId === 'default') return ensureSpeakerEmbeddings();
  if (voiceEmbeddingsCache.has(voiceId)) return voiceEmbeddingsCache.get(voiceId);
  const binPath = path.join(DATA_DIR, `speaker_${voiceId}.bin`);
  if (fs.existsSync(binPath)) {
    const buf = fs.readFileSync(binPath);
    const emb = new Float32Array(new Uint8Array(buf).buffer);
    voiceEmbeddingsCache.set(voiceId, emb);
    return emb;
  }
  const offset = SPEAKER_OFFSETS[voiceId];
  if (offset === undefined) return ensureSpeakerEmbeddings();
  const url = `${DATASET_API}&offset=${offset}&length=${SAMPLES_TO_AVERAGE}`;
  const resp = await fetch(url);
  if (!resp.ok) throw new Error('Failed to fetch voice embeddings for ' + voiceId);
  const data = await resp.json();
  const avg = new Float32Array(512);
  let count = 0;
  for (const item of data.rows) {
    const match = item.row.filename.match(/cmu_us_(\w+)_arctic/);
    if (match && match[1] === voiceId) {
      for (let i = 0; i < 512; i++) avg[i] += item.row.xvector[i];
      count++;
    }
  }
  if (count === 0) return ensureSpeakerEmbeddings();
  for (let i = 0; i < 512; i++) avg[i] /= count;
  if (!fs.existsSync(DATA_DIR)) fs.mkdirSync(DATA_DIR, { recursive: true });
  fs.writeFileSync(binPath, Buffer.from(avg.buffer));
  voiceEmbeddingsCache.set(voiceId, avg);
  return avg;
}

function getVoices() {
  return VOICE_CATALOG;
}

async function getSTT() {
  if (sttPipeline) return sttPipeline;
  if (sttLoading) {
    while (sttLoading) await new Promise(r => setTimeout(r, 100));
    if (!sttPipeline) throw new Error('STT pipeline failed to load');
    return sttPipeline;
  }
  sttLoading = true;
  try {
    const { pipeline, env } = await loadTransformers();
    const modelPath = whisperModelPath();
    const isLocal = !modelPath.includes('/') || fs.existsSync(modelPath);
    env.allowLocalModels = true;
    env.allowRemoteModels = !isLocal;
    if (isLocal) env.localModelPath = '';
    sttPipeline = await pipeline('automatic-speech-recognition', modelPath, {
      device: 'cpu',
      local_files_only: isLocal,
    });
    return sttPipeline;
  } catch (err) {
    sttPipeline = null;
    throw new Error('STT model load failed: ' + err.message);
  } finally {
    sttLoading = false;
  }
}

async function getTTS() {
  if (ttsPipeline) return ttsPipeline;
  if (ttsLoading) {
    while (ttsLoading) await new Promise(r => setTimeout(r, 100));
    if (!ttsPipeline) throw new Error('TTS pipeline failed to load');
    return ttsPipeline;
  }
  ttsLoading = true;
  try {
    const { pipeline, env } = await loadTransformers();
    env.allowRemoteModels = true;
    ttsPipeline = await pipeline('text-to-speech', 'Xenova/speecht5_tts', {
      device: 'cpu',
      dtype: 'fp32',
    });
    await ensureSpeakerEmbeddings();
    return ttsPipeline;
  } catch (err) {
    ttsPipeline = null;
    throw new Error('TTS model load failed: ' + err.message);
  } finally {
    ttsLoading = false;
  }
}

function decodeWavToFloat32(buffer) {
  const view = new DataView(buffer.buffer || buffer);
  const riff = String.fromCharCode(view.getUint8(0), view.getUint8(1), view.getUint8(2), view.getUint8(3));
  if (riff !== 'RIFF') throw new Error('Not a WAV file');
  const numChannels = view.getUint16(22, true);
  const sampleRate = view.getUint32(24, true);
  const bitsPerSample = view.getUint16(34, true);
  let dataOffset = 44;
  for (let i = 36; i < view.byteLength - 8; i++) {
    if (view.getUint8(i) === 0x64 && view.getUint8(i+1) === 0x61 &&
        view.getUint8(i+2) === 0x74 && view.getUint8(i+3) === 0x61) {
      dataOffset = i + 8;
      break;
    }
  }
  const bytesPerSample = bitsPerSample / 8;
  const numSamples = Math.floor((view.byteLength - dataOffset) / (bytesPerSample * numChannels));
  const audio = new Float32Array(numSamples);
  for (let i = 0; i < numSamples; i++) {
    const offset = dataOffset + i * bytesPerSample * numChannels;
    if (bitsPerSample === 16) {
      audio[i] = view.getInt16(offset, true) / 32768;
    } else if (bitsPerSample === 32) {
      audio[i] = view.getFloat32(offset, true);
    } else {
      audio[i] = (view.getUint8(offset) - 128) / 128;
    }
  }
  return { audio, sampleRate };
}

function resampleTo16k(audio, fromRate) {
  if (fromRate === SAMPLE_RATE_STT) return audio;
  const ratio = fromRate / SAMPLE_RATE_STT;
  const newLen = Math.round(audio.length / ratio);
  const result = new Float32Array(newLen);
  for (let i = 0; i < newLen; i++) {
    const srcIdx = i * ratio;
    const lo = Math.floor(srcIdx);
    const hi = Math.min(lo + 1, audio.length - 1);
    const frac = srcIdx - lo;
    result[i] = audio[lo] * (1 - frac) + audio[hi] * frac;
  }
  return result;
}

function encodeWav(float32Audio, sampleRate) {
  const numSamples = float32Audio.length;
  const bytesPerSample = 2;
  const dataSize = numSamples * bytesPerSample;
  const buffer = new ArrayBuffer(44 + dataSize);
  const view = new DataView(buffer);
  const writeStr = (off, str) => { for (let i = 0; i < str.length; i++) view.setUint8(off + i, str.charCodeAt(i)); };
  writeStr(0, 'RIFF');
  view.setUint32(4, 36 + dataSize, true);
  writeStr(8, 'WAVE');
  writeStr(12, 'fmt ');
  view.setUint32(16, 16, true);
  view.setUint16(20, 1, true);
  view.setUint16(22, 1, true);
  view.setUint32(24, sampleRate, true);
  view.setUint32(28, sampleRate * bytesPerSample, true);
  view.setUint16(32, bytesPerSample, true);
  view.setUint16(34, 16, true);
  writeStr(36, 'data');
  view.setUint32(40, dataSize, true);
  for (let i = 0; i < numSamples; i++) {
    const s = Math.max(-1, Math.min(1, float32Audio[i]));
    view.setInt16(44 + i * 2, s < 0 ? s * 32768 : s * 32767, true);
  }
  return Buffer.from(buffer);
}

async function transcribe(audioBuffer) {
  const buf = Buffer.isBuffer(audioBuffer) ? audioBuffer : Buffer.from(audioBuffer);
  if (buf.length < MIN_WAV_SIZE) {
    throw new Error('Audio too short (' + buf.length + ' bytes)');
  }
  let audio;
  const isWav = buf.length > 4 && buf.toString('ascii', 0, 4) === 'RIFF';
  if (isWav) {
    let decoded;
    try {
      decoded = decodeWavToFloat32(buf);
    } catch (err) {
      throw new Error('WAV decode failed: ' + err.message);
    }
    if (!decoded.audio || decoded.audio.length === 0) {
      throw new Error('WAV contains no audio samples');
    }
    audio = resampleTo16k(decoded.audio, decoded.sampleRate);
  } else {
    const sampleCount = Math.floor(buf.byteLength / 4);
    if (sampleCount === 0) throw new Error('Audio buffer too small');
    const aligned = new ArrayBuffer(sampleCount * 4);
    new Uint8Array(aligned).set(buf.subarray(0, sampleCount * 4));
    audio = new Float32Array(aligned);
  }
  if (audio.length < 100) {
    throw new Error('Audio too short for transcription');
  }
  const stt = await getSTT();
  let result;
  try {
    result = await stt(audio);
  } catch (err) {
    throw new Error('Transcription engine error: ' + err.message);
  }
  if (!result || typeof result.text !== 'string') {
    return '';
  }
  return result.text;
}

function splitSentences(text) {
  const raw = text.match(/[^.!?]+[.!?]+[\s]?|[^.!?]+$/g);
  if (!raw) return [text];
  return raw.map(s => s.trim()).filter(s => s.length > 0);
}

function cachePut(key, buf) {
  if (ttsCache.size >= TTS_CACHE_MAX) {
    const oldest = ttsCache.keys().next().value;
    ttsCache.delete(oldest);
  }
  ttsCache.set(key, buf);
}

async function synthesize(text, voiceId) {
  const cacheKey = (voiceId || 'default') + ':' + text;
  const cached = ttsCache.get(cacheKey);
  if (cached) {
    ttsCache.delete(cacheKey);
    ttsCache.set(cacheKey, cached);
    return cached;
  }
  const tts = await getTTS();
  const embeddings = await loadVoiceEmbedding(voiceId);
  const result = await tts(text, { speaker_embeddings: embeddings });
  const wav = encodeWav(result.audio, result.sampling_rate || SAMPLE_RATE_TTS);
  cachePut(cacheKey, wav);
  return wav;
}

async function* synthesizeStream(text, voiceId) {
  const sentences = splitSentences(text);
  const tts = await getTTS();
  const embeddings = await loadVoiceEmbedding(voiceId);
  for (const sentence of sentences) {
    const cacheKey = (voiceId || 'default') + ':' + sentence;
    const cached = ttsCache.get(cacheKey);
    if (cached) {
      ttsCache.delete(cacheKey);
      ttsCache.set(cacheKey, cached);
      yield cached;
      continue;
    }
    const result = await tts(sentence, { speaker_embeddings: embeddings });
    const wav = encodeWav(result.audio, result.sampling_rate || SAMPLE_RATE_TTS);
    cachePut(cacheKey, wav);
    yield wav;
  }
}

function getStatus() {
  return {
    sttReady: !!sttPipeline,
    ttsReady: !!ttsPipeline,
    sttLoading,
    ttsLoading,
  };
}

export { transcribe, synthesize, synthesizeStream, getSTT, getTTS, getStatus, getVoices };
